---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>
 
<!-- # Short Bio  -->
# Biography

I am currently a Research Scientist at [Shanghai AI Lab](https://www.shlab.org.cn/) and an Adjunct Ph.D. Supervisor at the School of AI, Shanghai Jiao Tong University. My research interests focus on **Multimodal Large Language Models**, **Next-Generation Document Understanding**, and **Data-Centric AI**.

I believe that true innovation stems from deep diving, and more importantly, from the relentless refinement and bold reshaping of existing technologies. **Refusing to settle for the status quo**, my goal is to deliver research that is not only scientifically rigorous but also practically transformativeâ€”tackling the "hard problems" others cannot, to provide unique solutions for the industry's most critical challenges.

Guided by this philosophy, I lead the R&D of **[MinerU](https://github.com/opendatalab/MinerU)**, an open-source toolkit for high-quality document parsing. The project has garnered over **50k GitHub stars** in just 1.5 years, frequently topping GitHub Trending charts. It is widely adopted by both academia and industry, serving as a **mainstream solution** for enterprises and developers building high-quality **LLM and RAG corpora**. Additionally, I have published over 40 papers in top-tier conferences such as CVPR, ICCV, NeurIPS, and ICLR, with over 4,000 Google Scholar citations.

---

æˆ‘æ˜¯ä¸Šæµ·äººå·¥æ™ºèƒ½å®éªŒå®¤ï¼ˆShanghai AI Labï¼‰çš„é’å¹´ç§‘å­¦å®¶ï¼Œä¸Šæµ·äº¤é€šå¤§å­¦äººå·¥æ™ºèƒ½å­¦é™¢å…¼èŒåšå£«ç”Ÿå¯¼å¸ˆï¼Œå…¥é€‰ä¸Šæµ·å¸‚ä¸œæ–¹è‹±æ‰æ‹”å°–äººæ‰é¡¹ç›®ã€‚æˆ‘çš„ç ”ç©¶èšç„¦äº**å¤šæ¨¡æ€å¤§æ¨¡å‹**ã€**ä¸‹ä¸€ä»£æ™ºèƒ½æ–‡æ¡£ç†è§£**ä»¥åŠ**ä»¥æ•°æ®ä¸ºä¸­å¿ƒçš„äººå·¥æ™ºèƒ½ï¼ˆData-Centric AIï¼‰**ã€‚

æˆ‘ç›¸ä¿¡çœŸæ­£çš„åˆ›æ–°æºäºæ·±è€•ï¼Œæ›´æºäºå¯¹ç°æœ‰æŠ€æœ¯çš„æè‡´æ‰“ç£¨ä¸å‹‡æ•¢é‡å¡‘ã€‚æˆ‘ä¸å›¿äºæ—¢æœ‰çš„æŠ€æœ¯è¾¹ç•Œï¼Œè€Œæ˜¯è‡´åŠ›äºäº§å‡ºæ—¢å…·å¤‡ç§‘å­¦ä¸¥è°¨æ€§ï¼Œåˆå…·æœ‰å˜é©æ„ä¹‰çš„ç ”ç©¶â€”â€”é€šè¿‡æ”»å…‹é‚£äº›åˆ«äººåšä¸åˆ°çš„éš¾é¢˜ï¼Œä¸ºè¡Œä¸šæœ€å…³é”®çš„æŒ‘æˆ˜æä¾›ç‹¬ä¸€æ— äºŒçš„è§£å†³æ–¹æ¡ˆã€‚ç§‰æŒè¿™ä¸€ç†å¿µï¼Œæˆ‘ä¸»å¯¼ç ”å‘äº†å¼€æºæ–‡æ¡£è§£æå·¥å…· **[MinerU](https://github.com/opendatalab/MinerU)**ã€‚è¯¥é¡¹ç›®åœ¨ä¸€å¹´åŠå†…æ–©è· **50k+ GitHub Stars**ï¼Œå¤šæ¬¡ç™»é¡¶ GitHub Trending å…¨çƒæ¦œå•ï¼Œä¸ä»…åœ¨å­¦æœ¯ç•Œå¹¿å—å¥½è¯„ï¼Œæ›´è¢«äº§ä¸šç•Œå¹¿æ³›é‡‡ç”¨ï¼Œæˆä¸ºä¼—å¤šä¼ä¸šä¸å¼€å‘è€…æ„å»ºé«˜è´¨é‡å¤§æ¨¡å‹è¯­æ–™åŠ RAG è¯­æ–™åº“çš„ä¸»æµé€‰æ‹©ã€‚åŒæ—¶ï¼Œæˆ‘åœ¨ CVPR, ICCV, NeurIPS, ICLR ç­‰é¡¶çº§ä¼šè®®å‘è¡¨è®ºæ–‡ 40 ä½™ç¯‡ï¼Œè°·æ­Œå­¦æœ¯å¼•ç”¨è¶… 4000 æ¬¡ã€‚

æˆ‘ä»¬æŒç»­å¯»æ‰¾ä¼˜ç§€çš„åšå£«ç”Ÿï¼ˆä¸Šäº¤ç­‰é¡¶å°–é«˜æ ¡åšå£«è”åŸ¹åé¢ï¼‰ã€åšå£«åç ”ç©¶å‘˜ã€å®ä¹ ç”ŸåŠå…¨èŒç ”ç©¶äººå‘˜ï¼Œå¦‚æœä½ å¯¹äººå·¥æ™ºèƒ½æ–¹å‘å……æ»¡çƒ­æƒ…ï¼Œè‡ªé©±åŠ›å¼ºï¼Œæ¬¢è¿ç”µå­é‚®ä»¶è”ç³»åŠ å…¥æˆ‘ä»¬ã€‚

ğŸ“§ **Email:** ictwangbin@gmail.com / wangbin@pjlab.org.cn

# ä¸ºä»€ä¹ˆåŠ å…¥æˆ‘ä»¬ï¼Ÿ

**1. åšæœ‰ä»·å€¼çš„å‰æ²¿ç ”ç©¶**  
æˆ‘ä»¬çš„æ–¹å‘â€”â€”å¤šæ¨¡æ€å¤§æ¨¡å‹ã€æ™ºèƒ½æ–‡æ¡£è§£æå’Œ Data-Centric AIï¼Œæ˜¯é€šå¾€ AGI çš„å¿…ç»ä¹‹è·¯ã€‚æˆ‘ä¸æƒ³å¸¦å¤§å®¶ä¸ºäº†å‘è®ºæ–‡è€Œå‘è®ºæ–‡ï¼Œæˆ–è€…åšä½æ°´å¹³çš„é‡å¤å»ºè®¾ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬åªåšä¸¤ä»¶äº‹ï¼šè¦ä¹ˆè§£å†³äº§ä¸šç•Œæœ€æ£˜æ‰‹çš„ç—›ç‚¹ï¼Œè¦ä¹ˆæŒ‘æˆ˜å­¦æœ¯ç•Œæœªè§£çš„éš¾é¢˜ã€‚**è¦åšï¼Œå°±åšèƒ½è¢«åŒè¡Œè®°ä½ã€è¢«å¼€å‘è€…çœŸæ­£ä½¿ç”¨çš„å·¥ä½œã€‚**

**2. å……è¶³çš„èµ„æºä¸çº¯ç²¹çš„æ°›å›´**  
ç®—åŠ›æ˜¯åšå¤§æ¨¡å‹ç ”ç©¶çš„åº•æ°”ã€‚å®éªŒå®¤æ‹¥æœ‰å……è¶³çš„ GPU é›†ç¾¤ï¼Œä½ ä¸å¿…å› ä¸ºèµ„æºå—é™è€Œç¼©æ‰‹ç¼©è„šï¼Œå¯ä»¥å¤§èƒ†å»éªŒè¯é‚£äº›æ˜‚è´µçš„æƒ³æ³•ã€‚ç»„é‡Œçš„å°ä¼™ä¼´å‡æ¥è‡ªé¡¶å°–é«˜æ ¡ï¼Œå¤§å®¶å¹´é¾„ç›¸ä»¿ï¼Œç§‘ç ”æ°›å›´å¾ˆçº¯ç²¹ã€‚å¯¹äºåšå£«ç”Ÿå’Œå®ä¹ ç”Ÿï¼Œå®éªŒå®¤æä¾›ä¸é”™çš„æ´¥è´´ï¼Œè®©å¤§å®¶å¯ä»¥ä¸“æ³¨äºæŠ€æœ¯çªç ´ã€‚

**3. äº¦å¸ˆäº¦å‹ï¼Œå…¨æµç¨‹æŒ‡å¯¼**  
æˆ‘è‡ªå·±ä¹Ÿæ˜¯ä»å­¦ç”Ÿè¿‡æ¥çš„ï¼Œæ·±çŸ¥å¤§å®¶åœ¨ä¸åŒé˜¶æ®µçš„ç—›ç‚¹ï¼Œæ‰€ä»¥æˆ‘æ‹’ç»â€œæ”¾å…»â€ã€‚
*   **å®šåˆ¶åŒ–åŸ¹å…»**ï¼šæˆ‘ä¸ä¼šå½“â€œç”©æ‰‹æŒæŸœâ€ã€‚ä»é€‰é¢˜ã€Coding åˆ°å†™ Paperï¼Œæˆ‘ä¼šæä¾›ä¸€å¯¹ä¸€æŒ‡å¯¼ï¼Œå¹¶æ ¹æ®ä½ çš„ç‰¹é•¿è§„åˆ’è·¯çº¿ã€‚
*   **æ¸…æ™°çš„è·¯å¾„**ï¼šæˆ‘ä»¬æ¯å‘¨éƒ½æœ‰å‰æ²¿ Paper Readingã€‚æˆ‘çš„ç›®æ ‡å¾ˆæ˜ç¡®ï¼šå¸¦ä½ èµ°å®Œä»ã€å¤¯å®åŸºç¡€ã€‘åˆ°ã€ç‹¬ç«‹å‘é¡¶ä¼šã€‘ï¼Œå†åˆ°ã€åšå‡ºå½±å“åŠ›å·¥ä½œã€‘çš„å…¨è¿‡ç¨‹ï¼Œæœ€ç»ˆæŠŠä½ åŸ¹å…»æˆèƒ½ç‹¬å½“ä¸€é¢çš„ç ”ç©¶è€…ã€‚
*   **å¹³ç­‰çš„äº¤æµ**ï¼šä½œä¸ºé’å¹´å¯¼å¸ˆï¼Œæˆ‘ä»¬ä¹‹é—´æ²¡æœ‰ä»£æ²Ÿã€‚æ— è®ºæ˜¯ç§‘ç ”å¡å£³äº†ï¼Œè¿˜æ˜¯å¯¹æœªæ¥è¿·èŒ«äº†ï¼Œéšæ—¶éƒ½å¯ä»¥æ‰¾æˆ‘èŠã€‚

**4. æœŸå¾…ä½ çš„åŠ å…¥**  
ç›®å‰ 2026 çº§åšå£«ç”Ÿåé¢å·²æ»¡ï¼Œéå¸¸æ¬¢è¿ **2027 çº§ç›´åšç”Ÿã€æ™®åšç”Ÿ** æå‰è”ç³»æ¥ç»„é‡Œå®ä¹ ã€‚
åšç§‘ç ”æ˜¯ä¸€åœºé•¿è·‘ï¼Œå¸Œæœ›èƒ½æ‰¾åˆ°å¿—åŒé“åˆçš„ä½ ï¼Œ**ä¸€èµ·åœ¨å¤§æ¨¡å‹æ—¶ä»£åšç‚¹ä¸ä¸€æ ·çš„äº‹æƒ…ã€‚**
*(æ³¨ï¼šå¯¹äºå·²ç»æ¯•ä¸šçš„ä¼˜ç§€ç ”ç©¶äººå‘˜ï¼Œå¦‚æœæ¸´æœ›åœ¨å…·æœ‰å½±å“åŠ›çš„å¹³å°ä¸Šæ–½å±•æ‹³è„šï¼ŒåŒæ ·æ¬¢è¿è”ç³»åŠ å…¥ã€‚)*




# ğŸ”¥ News

### 2025:
- *2025.09*: &nbsp;ğŸ‰ğŸ‰  MinerU 2.5 is released! A 1.2B vision-language model for document parsing. [[Tech Report]](https://arxiv.org/abs/2509.22186) [[Hugging Face Model]](https://huggingface.co/opendatalab/MinerU2.5-2509-1.2B) [[GitHub]](https://github.com/opendatalab/MinerU/)   
  - SOTA Performance: Surpasses general models (Gemini 2.5-Pro, GPT-4o, etc.) and specialized tools (MonkeyOCR, PP-StructureV3).  
  - High Efficiency: Achieves top accuracy with significantly greater speed than large-model solutions.  

- *2025.06*: &nbsp;ğŸ‰ğŸ‰ OHR, LEGION and Chimera are accepted by ICCV 2025.
- *2025.02*: &nbsp;ğŸ‰ğŸ‰ OmniDocBench and CDM are accepted by CVPR 2025.
- *2025.01*: &nbsp;ğŸ‰ğŸ‰ GeoX and OmniCorpus are accepted by ICLR 2025.

### 2024:
- *2024.09*: &nbsp;ğŸ‰ğŸ‰ InternLM-XComposer2-4KHD is accepted by NeurIPS 2024.
- *2024.07*: &nbsp;ğŸ”¥ğŸ”¥ğŸ”¥ [<img src="images/pdf-extract-kit_logo.png" width="150px" style="vertical-align:bottom;">](https://github.com/opendatalab/PDF-Extract-Kit) has received **<span style="color:red">3500+</span>** GitHub stars within one month.
- *2024.07*: &nbsp;ğŸ”¥ğŸ”¥ğŸ”¥ [<img src="images/mineru-logo.png" width="90px" style="vertical-align:bottom;">](https://github.com/opendatalab/MinerU) has received **<span style="color:red">4200+</span>** GitHub stars and **<span style="color:red">ranked #1</span>** on the GitHub Trending list.
- *2024.07*: &nbsp;ğŸ‰ğŸ‰ CLIP-Parrot-Bias is accepted by ECCV 2024 (<span style="color:red">**Oral**</span>).
- *2024.02*: &nbsp;ğŸ‰ğŸ‰ OPERA is accepted by CVPR 2024. 
- *2023.12*: &nbsp;ğŸ‰ğŸ‰ VIGC is accepted by AAAI 2024. 
- *2023.12*: &nbsp;ğŸ‰ğŸ‰ One paper is accepted by IJAEOG 2024. 
- *2023.08*: &nbsp;ğŸ‰ğŸ‰ DropQueries is accepted by TMM 2023. 
- *2023.08*: &nbsp;ğŸ‰ğŸ‰ V3Det is accepted by ICCV 2023 (<span style="color:red">**Oral**</span>). 
<!-- - *2022.07*: &nbsp;ğŸ‰ğŸ‰ PG-MPI is accepted by ECCV2022. -->

# ğŸš€ Project


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Github Repo</div><img src='images/mineru2_5.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[**MinerU2.5**: A Decoupled Vision-Language Model for Efficient High-Resolution Document Parsing <strong>(Project Lead)</strong>](https://arxiv.org/abs/2509.22186)| 
 [**Demo(Hugging Face)**](https://huggingface.co/spaces/opendatalab/MinerU) \| 
 [**Models(Hugging Face)**](https://huggingface.co/opendatalab/MinerU2.5-2509-1.2B) \| [**Models(ModelScope)**](https://modelscope.cn/models/OpenDataLab/MinerU2.5-2509-1.2B) \| [**Github** ![](https://img.shields.io/github/stars/opendatalab/MinerU)](https://github.com/opendatalab/MinerU)
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Github Repo</div><img src='images/pdf-extract-kit-pipeline.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**PDF-Extract-Kit**: A Comprehensive Toolkit for High-Quality PDF Content Extraction <strong>(Project Lead)</strong>

[**Models(Hugging Face)**](https://huggingface.co/wanderkid/PDF-Extract-Kit) \| [**Models(ModelScope)**](https://www.modelscope.cn/models/wanderkid/PDF-Extract-Kit) \| [**Github** ![](https://img.shields.io/github/stars/opendatalab/PDF-Extract-Kit)](https://github.com/opendatalab/PDF-Extract-Kit)
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Github Repo</div><img src='images/mineru-pipeline.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[**MinerU**: An Open-Source Solution for Precise Document Content Extraction <strong>(Project Lead)</strong>](https://arxiv.org/abs/2409.18839)
 <a href="https://trendshift.io/repositories/11174" target="_blank"><img src="https://trendshift.io/api/badge/repositories/11174" alt="opendatalab%2FMinerU | Trendshift" style="width: 200px; height: 55px;"/></a> | [**Github** ![](https://img.shields.io/github/stars/opendatalab/MinerU)](https://github.com/opendatalab/MinerU)
</div>
</div>

# ğŸ“ Publications 

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ECCV 2024</div><img src='images/eccv2024_clip-parrot-bias.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Parrot Captions Teach CLIP to Spot Text](https://arxiv.org/abs/2312.14232)

Yiqi Lin<sup>*</sup>, Conghui He<sup>*</sup>, Alex Jinpeng Wang<sup>*</sup>, **Bin Wang**<sup>*</sup>, Weijia Li, Mike Zheng Shou

ECCV 2024 <span style="color:red">**Oral**</span>, \| [**Project**](https://linyq17.github.io/CLIP-Parrot-Bias/) \| [**Github** ![](https://img.shields.io/github/stars/opendatalab/CLIP-Parrot-Bias)](https://github.com/opendatalab/CLIP-Parrot-Bias)
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">AAAI 2024</div><img src='images/AAAI2024_VIGC.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[VIGC: Visual Instruction Generation and Correction](https://arxiv.org/abs/2308.12714)

**Bin Wang**, Fan Wu, Xiao Han, Jiahui Peng, Huaping Zhong, Pan Zhang, Xiaoyi Dong, Weijia Li, Wei Li, Jiaqi Wang, Conghui He

AAAI 2024, \| [**Project**](https://opendatalab.github.io/VIGC/) \| [**Github** ![](https://img.shields.io/github/stars/opendatalab/VIGC)](https://github.com/opendatalab/VIGC)
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">IJAEOG 2024</div><img src='images/IJAEOG.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Exploring the user guidance for more accurate building segmentation from high-resolution remote sensing images](https://www.sciencedirect.com/science/article/pii/S1569843223004338)

Dinghao Yang<sup>*</sup>, **Bin Wang**<sup>*</sup>, Weijia Li, Conghui He

IJAEOG 2024,  \| [**Github** ![](https://img.shields.io/github/stars/StephenDHYang/UGBS-pytorch)](https://github.com/StephenDHYang/UGBS-pytorch)
</div>
</div>



<div class='paper-box'><div class='paper-box-image'><div><div class="badge">TMM 2023</div><img src='images/DropQuery.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[DropQueries: A Simple Way to Discover Comprehensive Segment Representations](https://scholar.google.com/citations?view_op=view_citation&hl=zh-CN&user=WljXYoYAAAAJ&sortby=pubdate&citation_for_view=WljXYoYAAAAJ:ufrVoPGSRksC)

Haojie Ding, **Bin Wang**, Guoliang Kang, Weijia Li, Conghui He, Yao Zhao, and Yunchao Wei

TMM 2023 
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICCV 2023</div><img src='images/V3Det.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[V3Det: Vast Vocabulary Visual Detection Dataset](https://arxiv.org/abs/2304.03752)

Jiaqi Wang, Pan Zhang, Tao Chu, Yuhang Cao, Yujie Zhou, Tong Wu, **Bin Wang**, Conghui He, and Dahua Lin

ICCV 2023 <span style="color:red">**Oral**</span>, \| [**Project**](https://v3det.openxlab.org.cn/) \| [**Github** ![](https://img.shields.io/github/stars/V3Det/V3Det?style=social)](https://github.com/V3Det/V3Det) 

</div>
</div>



<div class='paper-box'><div class='paper-box-image'><div><div class="badge">IJCAI 2019</div><img src='images/IJCAI2019_BPG.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Boundary perception guidance: A scribble-supervised semantic segmentation approach](https://opus.lib.uts.edu.au/bitstream/10453/141475/2/0508.pdf)

**Bin Wang**, Guojun Qi, Sheng Tang, Tianzhu Zhang, Yunchao Wei, Linghui Li, and Yongdong Zhang

IJCAI 2019
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">MICCAI 2019</div><img src='images/MICCAI2019.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Spatiotemporal Breast Mass Detection Network(MD-Net) in 4D DCE-MRI Images](https://scholar.google.com/citations?view_op=view_citation&hl=zh-CN&user=WljXYoYAAAAJ&citation_for_view=WljXYoYAAAAJ:u5HHmVD_uO8C)

Lixi Deng, Sheng Tang, Huazhu Fu, **Bin Wang**, and Yongdong Zhang

MICCAI 2019
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">MICCAI 2018</div><img src='images/MICCAI2018_Nodule.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Automated pulmonary nodule detection: High sensitivity with few candidates](https://scholar.google.com/citations?view_op=view_citation&hl=zh-CN&user=WljXYoYAAAAJ&citation_for_view=WljXYoYAAAAJ:2osOgNQ5qMEC)

**Bin Wang**, Guojun Qi, Sheng Tang, Liheng Zhang, Lixi Deng, and Yongdong Zhang

MICCAI 2018
</div>
</div>



<!-- <div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2021</div><img src='images/cocosnet_v2.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[CoCosNet v2: Full-Resolution Correspondence Learning for Image Translation](https://arxiv.org/pdf/2012.02047.pdf)

Xingran Zhou, Bo Zhang, Ting Zhang, **Pan Zhang**, Jianmin Bao, Dong Chen, Zhongfei Zhang, Fang Wen

CVPR 2021 <span style="color:red">**Oral**</span>, [<span style="color:red">**Best Paper Candidate**</span>](https://cvpr2021.thecvf.com/node/290) \| [**Github** ![](https://img.shields.io/github/stars/microsoft/CoCosNet-v2?style=social)](https://github.com/microsoft/CoCosNet-v2) \| [**Slides**](https://www.dropbox.com/s/g7dezxm2mhw6gqo/CoCosNet%20slides.pptx?dl=0)

</div>
</div> -->




<!-- - [Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet](https://github.com), A, B, C, **CVPR 2020** -->

# ğŸ– Honors and Awards
- *2020.06*, Zhu Li Yuehua Outstanding Ph.D. student Scholarship, Chinese Academy of Sciences (CAS).
- *2016.09*, Won 3rd place in the ILSVRC 2016 VID task (Object Detection from Video).
 
<!-- - *2017.06*, Honor Ranking of Talent Program in Information Science and Technology (For top 5% students by USTC). 
- *2015.06*, National Scholarship (The highest scholarship awarded by the Ministry of Education, China).
- *2014.06*, National Scholarship (The highest scholarship awarded by the Ministry of Education, China). -->

# ğŸ¢ Work Experience
- *2020.07 - 2022.08*, Researcher, SenseTime, Shenzhen, China.


# ğŸ“– Education 
- *2015.09 - 2020.06*, Ph.D., University of Chinese Academy of Sciences, Beijing, China.
- *2013.09 - 2015.06*, M.S., Beijing Jiaotong University, Beijing, China.